{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mEVt8IQjdT2H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  MNIST digits classification with  Keras \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QX4p7Q5Udm8O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "print(\"We're using TF\", tf.__version__)\n",
        "import keras\n",
        "print(\"We are using Keras\", keras.__version__)\n",
        "\n",
        "\n",
        "from importlib import reload\n",
        "\n",
        "import keras_utils\n",
        "from keras_utils import reset_tf_session"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z71aIGWTeKMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Look at the data**\n",
        "In this task we have 50000 28x28 images of digits from 0 to 9. We will train a classifier on this data."
      ]
    },
    {
      "metadata": {
        "id": "ngiv9sk3eV74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "73f9d1ea-13fa-44ec-f5c2-26ef60308938"
      },
      "cell_type": "code",
      "source": [
        "import preprocessed_mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-1c82c05a800c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CY4Q_8Ivezti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear model\n",
        ".\n",
        "\n",
        "Here's the plan:\n",
        "\n",
        "Flatten the images (28x28 -> 784) with X_train.reshape((X_train.shape[0], -1)) to simplify our linear model implementation\n",
        "Use a matrix placeholder for flattened X_train\n",
        "Convert y_train to one-hot encoded vectors that are needed for cross-entropy\n",
        "Use a shared variable W for all weights (a column  wk→wk→  per class) and b for all biases.\n",
        "Aim for ~0.93 validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "iMoQQvLwekWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "print(X_train_flat.shape)\n",
        "\n",
        "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
        "print(X_val_flat.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LjtYf_ZCgNfL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# one-hot encode the target\n",
        "import keras\n",
        "\n",
        "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
        "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
        "\n",
        "print(y_train_oh.shape)\n",
        "print(y_train_oh[:3], y_train[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lpFwWsqgg6cI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# building a model with keras\n",
        "from keras.layers import Dense,Activation\n",
        "from keras.models import Sequential\n",
        "# we still need to clear a graph though\n",
        "s=reset_tf_session()\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Dense(256,input_shape(784,)))  # the first layer must specify the input shape (replacing placeholders)\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(256))\n",
        "odel.add(Activation('sigmoid'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yb5-H2tPocLS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# you can look at all layers and parameter count\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZxR_9eIodJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now we \"compile\" the model specifying the loss and optimizer\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V7RevBmAzn3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# and now we can fit model with model.fit()\n",
        "model.fit(X_train_flat,y_train_oh,batch_size=512,epochs=40,validation_data=(X_val_flat,y_val_oh), callbacks=[keras_utils.TqdmProgressCallback()],\n",
        "    verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Af7-iVR80w6A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here're the notes for those who want to play around here\n",
        "Here are some tips on what you could do:\n",
        "\n",
        "Network size\n",
        "\n",
        "More neurons,\n",
        "More layers, (docs)\n",
        "\n",
        "Other nonlinearities in the hidden layers\n",
        "\n",
        "tanh, relu, leaky relu, etc\n",
        "Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
        "\n",
        "\n",
        "Early Stopping\n",
        "Training for 100 epochs regardless of anything is probably a bad idea.\n",
        "Some networks converge over 5 epochs, others - over 500.\n",
        "Way to go: stop when validation score is 10 iterations past maximum\n",
        "\n",
        "\n",
        "Faster optimization\n",
        "rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
        "Converge faster and sometimes reach better optima\n",
        "It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
        "Regularize to prevent overfitting\n",
        "Add some L2 weight norm to the loss function, theano will do the rest\n",
        "Can be done manually or via - https://keras.io/regularizers/\n",
        "\n",
        "\n",
        "Data augmemntation - getting 5x as large dataset for free is a great deal\n",
        "https://keras.io/preprocessing/image/\n",
        "Zoom-in+slice = move\n",
        "Rotate+zoom(to remove black stripes)\n",
        "any other perturbations\n",
        "Simple way to do that (if you have PIL/Image):\n",
        "from scipy.misc import imrotate,imresize\n",
        "and a few slicing\n",
        "Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
        "\n",
        "​"
      ]
    },
    {
      "metadata": {
        "id": "h_cNtj4u1T9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}